{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_neighbors(L):\n",
    "    \"\"\"Generate neighbor indices for a 2D lattice with periodic boundary conditions.\"\"\"\n",
    "    lattice = np.arange(L*L).reshape(L, L)\n",
    "    ups = np.roll(lattice, -1, axis=0)\n",
    "    rights = np.roll(lattice, -1, axis=1)\n",
    "    downs = np.roll(lattice, 1, axis=0)\n",
    "    lefts = np.roll(lattice, 1, axis=1)\n",
    "    return ups, rights, downs, lefts\n",
    "\n",
    "def montecarlo(L, T, nsweeps=10000000, measure_rate=5000):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo simulation for the 2D Ising model WITHOUT thermalization.\n",
    "    This version collects configurations starting from sweep 0 (non-equilibrium).\n",
    "    \n",
    "    Parameters:\n",
    "    - L: Lattice size (L x L)\n",
    "    - T: Temperature\n",
    "    - nsweeps: Total number of sweeps\n",
    "    - measure_rate: Frequency of configuration measurements\n",
    "    \n",
    "    Returns:\n",
    "    - List of spin configurations (includes non-equilibrium samples!)\n",
    "    \"\"\"\n",
    "    beta = 1/T\n",
    "    conf = np.random.choice([-1, 1], (L, L))  # Random initial state\n",
    "    confs = []  # NO thermalization - collects from beginning!\n",
    "    ups, rights, downs, lefts = get_neighbors(L)\n",
    "\n",
    "    for sweep in range(nsweeps):\n",
    "        for idx in np.ndindex(conf.shape):\n",
    "            i, j = idx\n",
    "            deltaE = 2.0 * conf[i,j] * (conf[ups[i,j] // L, ups[i,j] % L] +\n",
    "                                        conf[rights[i,j] // L, rights[i,j] % L] +\n",
    "                                        conf[downs[i,j] // L, downs[i,j] % L] +\n",
    "                                        conf[lefts[i,j] // L, lefts[i,j] % L])\n",
    "            # Metropolis criterion\n",
    "            if deltaE <= 0 or np.random.random() < np.exp(-beta * deltaE):\n",
    "                conf[i, j] *= -1  # Flip spin\n",
    "\n",
    "        # Store configurations from the start (NO thermalization period)\n",
    "        if sweep % measure_rate == 0:\n",
    "            confs.append(conf.copy())\n",
    "\n",
    "    return confs\n",
    "\n",
    "def generate_ising_dataset(L=20, n_configs_per_temp=100):\n",
    "    \"\"\"\n",
    "    Generate Ising model configurations WITHOUT thermalization.\n",
    "    WARNING: This will produce biased, non-equilibrium data!\n",
    "    \n",
    "    Parameters:\n",
    "    - L: Lattice size\n",
    "    - n_configs_per_temp: Number of configurations to generate per temperature\n",
    "    \n",
    "    Returns:\n",
    "    - X: Array of flattened configurations (n_samples, L*L)\n",
    "    - y: Array of labels (0=ordered, 1=disordered)\n",
    "    - temperatures: Array of temperatures used\n",
    "    \"\"\"\n",
    "    Tc = 2.269  # Critical temperature for 2D Ising model\n",
    "    \n",
    "    # Define temperature ranges\n",
    "    # Below Tc: ordered phase\n",
    "    T_ordered = np.linspace(1.5, 2.1, 6)\n",
    "    # Near Tc: critical region\n",
    "    T_critical = np.linspace(2.15, 2.4, 3)\n",
    "    # Above Tc: disordered phase\n",
    "    T_disordered = np.linspace(2.5, 3.5, 6)\n",
    "    \n",
    "    all_temps = np.concatenate([T_ordered, T_critical, T_disordered])\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    temperatures = []\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    print(\"=\"*60)\n",
    "    print(\"WARNING: NO THERMALIZATION - COLLECTING NON-EQUILIBRIUM DATA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Started data generation: {start_time.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"Critical temperature Tc = {Tc}\")\n",
    "    print(f\"Generating {n_configs_per_temp} configurations at each of {len(all_temps)} temperatures\")\n",
    "    print(f\"Lattice size: {L} x {L}\\n\")\n",
    "    \n",
    "    for idx, T in enumerate(all_temps):\n",
    "        print(f\"Temperature {idx+1}/{len(all_temps)}: T = {T:.3f}\", end=\" \")\n",
    "        \n",
    "        # Determine label\n",
    "        if T < Tc:\n",
    "            label = 0  # Ordered\n",
    "            phase = \"ordered\"\n",
    "        else:\n",
    "            label = 1  # Disordered\n",
    "            phase = \"disordered\"\n",
    "        \n",
    "        print(f\"({phase})\")\n",
    "        \n",
    "        # Calculate required sweeps\n",
    "        measure_rate = 500\n",
    "        nsweeps = measure_rate * n_configs_per_temp\n",
    "        \n",
    "        # Generate configurations (NO thermalization!)\n",
    "        configs = montecarlo(L, T, nsweeps=nsweeps, measure_rate=measure_rate)\n",
    "        \n",
    "        # Flatten each configuration and add to dataset\n",
    "        for config in configs[:n_configs_per_temp]:\n",
    "            X.append(config.flatten())\n",
    "            y.append(label)\n",
    "            temperatures.append(T)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds() / 60\n",
    "    print(f\"\\nCompleted in {duration:.2f} minutes\")\n",
    "    print(f\"Total configurations: {len(X)}\")\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(temperatures)\n",
    "\n",
    "# Generate dataset WITHOUT thermalization\n",
    "print(\"=\"*60)\n",
    "print(\"ISING MODEL PHASE CLASSIFICATION - NO THERMALIZATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "X, y, temps = generate_ising_dataset(L=20, n_configs_per_temp=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce307b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Preparation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature dimension: {X.shape[1]} (from {int(np.sqrt(X.shape[1]))}x{int(np.sqrt(X.shape[1]))} lattice)\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Ordered (T < Tc):     {np.sum(y == 0)} samples ({100*np.sum(y == 0)/len(y):.1f}%)\")\n",
    "print(f\"  Disordered (T > Tc):  {np.sum(y == 1)} samples ({100*np.sum(y == 1)/len(y):.1f}%)\")\n",
    "\n",
    "# Analyze temperature distribution\n",
    "unique_temps = np.unique(temps)\n",
    "print(f\"\\nTemperature range: {unique_temps.min():.3f} to {unique_temps.max():.3f}\")\n",
    "print(f\"Number of unique temperatures: {len(unique_temps)}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(f\"  Mean magnetization (ordered):    {np.mean(X[y==0]):.4f}\")\n",
    "print(f\"  Mean magnetization (disordered): {np.mean(X[y==1]):.4f}\")\n",
    "print(f\"  Std (ordered):    {np.std(X[y==0]):.4f}\")\n",
    "print(f\"  Std (disordered): {np.std(X[y==1]):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTICE: Without thermalization, mean magnetizations may be\")\n",
    "print(\"similar for both classes due to non-equilibrium bias!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817683fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and Split Data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(len(X))\n",
    "X_shuffled = X[shuffle_idx]\n",
    "y_shuffled = y[shuffle_idx]\n",
    "temps_shuffled = temps[shuffle_idx]\n",
    "\n",
    "# Split into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test, temps_train, temps_test = train_test_split(\n",
    "    X_shuffled, y_shuffled, temps_shuffled, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_shuffled  # Ensure balanced class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({100*len(X_train)/len(X):.0f}%)\")\n",
    "print(f\"  Ordered:     {np.sum(y_train == 0)} samples\")\n",
    "print(f\"  Disordered:  {np.sum(y_train == 1)} samples\")\n",
    "print(f\"\\nTest set: {X_test.shape[0]} samples ({100*len(X_test)/len(X):.0f}%)\")\n",
    "print(f\"  Ordered:     {np.sum(y_test == 0)} samples\")\n",
    "print(f\"  Disordered:  {np.sum(y_test == 1)} samples\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nStandardization applied:\")\n",
    "print(f\"  Training mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Training std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "print(f\"  Test mean:     {X_test_scaled.mean():.6f}\")\n",
    "print(f\"  Test std:      {X_test_scaled.std():.6f}\")\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Configurations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Select representative configurations\n",
    "L = 20\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Sample Ising Model Configurations (NO THERMALIZATION)', \n",
    "             fontsize=16, fontweight='bold', color='red')\n",
    "\n",
    "# Show 3 ordered and 3 disordered configurations\n",
    "ordered_indices = np.where(y_train == 0)[0][:3]\n",
    "disordered_indices = np.where(y_train == 1)[0][:3]\n",
    "\n",
    "for i, idx in enumerate(ordered_indices):\n",
    "    config = X_train[idx].reshape(L, L)\n",
    "    temp = temps_train[idx]\n",
    "    axes[0, i].imshow(config, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0, i].set_title(f'\"Ordered\" (Non-equilibrium)\\nT = {temp:.3f} < Tc', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, idx in enumerate(disordered_indices):\n",
    "    config = X_train[idx].reshape(L, L)\n",
    "    temp = temps_train[idx]\n",
    "    axes[1, i].imshow(config, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[1, i].set_title(f'\"Disordered\" (Non-equilibrium)\\nT = {temp:.3f} > Tc', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete. Blue = spin -1, Red = spin +1\")\n",
    "print(\"\\nNOTE: These configurations may look similar between classes\")\n",
    "print(\"due to lack of thermalization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a53a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION CLASSIFIER\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train Logistic Regression with default parameters\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = clf.predict(X_train_scaled)\n",
    "y_pred_test = clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate metrics for training set (to check for overfitting)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_precision = precision_score(y_train, y_pred_train)\n",
    "train_recall = recall_score(y_train, y_pred_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "\n",
    "# Display results\n",
    "print(\"TRAINING SET PERFORMANCE:\")\n",
    "print(f\"  Accuracy:  {train_accuracy:.4f}\")\n",
    "print(f\"  Precision: {train_precision:.4f}\")\n",
    "print(f\"  Recall:    {train_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {train_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (Test Set):\")\n",
    "print(\"-\"*60)\n",
    "print(classification_report(y_test, y_pred_test, \n",
    "                          target_names=['Ordered (T<Tc)', 'Disordered (T>Tc)'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"-\"*60)\n",
    "print(\"CONFUSION MATRIX (Test Set):\")\n",
    "print(\"-\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"\\n                Predicted\")\n",
    "print(f\"              Ordered  Disordered\")\n",
    "print(f\"Actual Ordered    {cm[0,0]:3d}      {cm[0,1]:3d}\")\n",
    "print(f\"       Disordered {cm[1,0]:3d}      {cm[1,1]:3d}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['Ordered', 'Disordered'],\n",
    "            yticklabels=['Ordered', 'Disordered'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Logistic Regression\\n(NO THERMALIZATION)', \n",
    "          fontsize=14, fontweight='bold', color='red')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"SUMMARY: Test Accuracy = {test_accuracy:.2%}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEXPECTED: Poor accuracy due to non-equilibrium data!\")\n",
    "print(\"Without thermalization, the classifier cannot learn meaningful\")\n",
    "print(\"phase distinctions because the data is contaminated with\")\n",
    "print(\"non-equilibrium configurations from the random initial state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964c765",
   "metadata": {},
   "source": [
    "## Analysis: Impact of No Thermalization\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "This notebook demonstrates what happens when you **skip thermalization** in Monte Carlo simulations:\n",
    "\n",
    "#### **1. Non-Equilibrium Bias**\n",
    "\n",
    "**Problem:**\n",
    "- Initial state: Random configuration (50% up, 50% down spins)\n",
    "- Early samples collected before system reaches thermal equilibrium\n",
    "- Both \"ordered\" and \"disordered\" classes contain similar non-equilibrium states\n",
    "\n",
    "**Result:**\n",
    "- Mean magnetizations are similar between classes\n",
    "- Configurations look visually similar regardless of temperature\n",
    "- No clear phase distinction in the dataset\n",
    "\n",
    "#### **2. Poor Classification Performance**\n",
    "\n",
    "**Expected test accuracy: ~50-60%** (barely better than random guessing)\n",
    "\n",
    "**Why?**\n",
    "- The classifier correctly identifies that there's **no meaningful difference** between the two classes\n",
    "- Without equilibration, T=1.5 and T=3.5 configurations are statistically indistinguishable\n",
    "- Both classes are dominated by the random initial state\n",
    "\n",
    "#### **3. What This Demonstrates**\n",
    "\n",
    "This experiment proves that:\n",
    "\n",
    "✓ **Thermalization is essential** for generating valid statistical mechanics data\n",
    "\n",
    "✓ **Initial state matters**: Without discarding non-equilibrium samples, the data is biased\n",
    "\n",
    "✓ **Machine learning detects data quality**: Poor performance indicates contaminated data, not algorithm failure\n",
    "\n",
    "✓ **Physical validity ≠ numerical stability**: Code runs without errors but produces invalid physics\n",
    "\n",
    "### Comparison with Proper Thermalization\n",
    "\n",
    "| Metric | Without Thermalization | With Thermalization |\n",
    "|--------|------------------------|---------------------|\n",
    "| Test Accuracy | ~50-60% | ~95%+ (with proper parameters) |\n",
    "| Ordered Mean Mag | ~0.0 | >>0 (strong ferromagnetic order) |\n",
    "| Disordered Mean Mag | ~0.0 | ~0 (random) |\n",
    "| Visual Difference | Minimal | Dramatic |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**The poor performance in this notebook is EXPECTED and CORRECT!**\n",
    "\n",
    "It demonstrates that:\n",
    "- Without thermalization, Monte Carlo data is scientifically invalid\n",
    "- The classifier honestly reports that no phase distinction exists\n",
    "- This validates the importance of proper equilibration procedures\n",
    "\n",
    "**Always include thermalization in production Monte Carlo simulations!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
