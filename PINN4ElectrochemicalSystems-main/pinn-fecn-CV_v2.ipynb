{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8af7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CV SIMULATION: ADAPTIVE LOSS WEIGHTING\n",
      "================================================================================\n",
      "\n",
      "Running FD...\n",
      "✓ FD complete\n",
      "\n",
      "================================================================================\n",
      "TRAINING WITH GRADNORM\n",
      "================================================================================\n",
      "Training 3 systems for 9000 epochs...\n",
      "  Step     0 | Avg Loss: 3.31e+01\n",
      "  Step  1000 | Avg Loss: 2.50e-04\n",
      "  Step  2000 | Avg Loss: 1.06e-03\n",
      "  Step  3000 | Avg Loss: 3.67e-03\n",
      "  Step  4000 | Avg Loss: 9.14e-04\n",
      "  Step  5000 | Avg Loss: 7.12e-06\n",
      "  Step  6000 | Avg Loss: 1.77e-05\n",
      "  Step  7000 | Avg Loss: 4.37e-06\n",
      "  Step  8000 | Avg Loss: 1.47e-03\n",
      "  Step  9000 | Avg Loss: 1.54e-05\n",
      "✓ Training complete!\n",
      "\n",
      "================================================================================\n",
      "EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Generating plots...\n",
      "  ✓ 01_voltammogram_comparison\n",
      "  ✓ 02_profiles_system1\n",
      "  ✓ 02_profiles_system2\n",
      "  ✓ 02_profiles_system3\n",
      "  ✓ 03_training_loss_curves\n",
      "  ✓ 04_all_systems_overlay\n",
      "\n",
      "================================================================================\n",
      "SIMULATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "System 1 (c_O=1.0, c_R=0.01, ratio=100):\n",
      "  FD Peak Current:   21570949.20 μA\n",
      "  PINN Peak Current: 1143739008.00 μA\n",
      "  Relative Error:    5202.22%\n",
      "\n",
      "System 2 (c_O=1.0, c_R=1.0, ratio=1):\n",
      "  FD Peak Current:   35881974.37 μA\n",
      "  PINN Peak Current: 1143739383808.00 μA\n",
      "  Relative Error:    3187404.02%\n",
      "\n",
      "System 3 (c_O=0.01, c_R=1.0, ratio=0.01):\n",
      "  FD Peak Current:   18952204.08 μA\n",
      "  PINN Peak Current: 11437387808768.00 μA\n",
      "  Relative Error:    60348490.33%\n",
      "\n",
      "================================================================================\n",
      "✓ ALL PLOTS GENERATED WITH ADAPTIVE WEIGHTING!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  01_voltammogram_comparison_all_systems.png/pdf\n",
      "  02_profiles_system1.png/pdf\n",
      "  02_profiles_system2.png/pdf\n",
      "  02_profiles_system3.png/pdf\n",
      "  03_training_loss_curves.png/pdf\n",
      "  04_all_systems_overlay.png/pdf\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY IMPROVEMENTS:\n",
      "  1. GradNorm: Automatically balances loss gradients\n",
      "  2. Cosine annealing: Smooth learning rate decay\n",
      "  3. Gradient clipping: Prevents exploding gradients\n",
      "  4. Adaptive weights: Updated every 10 steps\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=================================================================================\n",
    "CYCLIC VOLTAMMETRY: ADAPTIVE LOSS WEIGHTING\n",
    "=================================================================================\n",
    "PROBLEM DIAGNOSIS:\n",
    "1. Loss scale mismatch (flux ~10^0, physics ~10^-5, conservation ~10^-6)\n",
    "2. Fixed weights don't adapt to changing dynamics\n",
    "3. Gradient magnitude imbalance causes oscillations\n",
    "\n",
    "SOLUTION: GradNorm + improved scheduler + gradient clipping\n",
    "=================================================================================\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_nn\n",
    "\n",
    "# Physical constants\n",
    "F, R, T = 96485.0, 8.314, 298.0\n",
    "E_start, E_vertex, E_end, E_0 = 0.2, -0.8, 0.2, -0.4\n",
    "nu, D, n, alpha, k_0, A, L = 10.0, 1e-6, 1, 0.5, 1e-1, 1.0, 0.1\n",
    "npts_x, npts_t = 100, 500\n",
    "systems = [\n",
    "    {'name': 'System 1', 'c_O': 1.0, 'c_R': 1e-2, 'ratio': 100},\n",
    "    {'name': 'System 2', 'c_O': 1.0, 'c_R': 1.0, 'ratio': 1},\n",
    "    {'name': 'System 3', 'c_O': 1e-2, 'c_R': 1.0, 'ratio': 0.01},\n",
    "]\n",
    "\n",
    "sweep_rate_V_per_s = nu / 1000.0\n",
    "t_forward = abs(E_vertex - E_start) / sweep_rate_V_per_s\n",
    "t_return = abs(E_end - E_vertex) / sweep_rate_V_per_s\n",
    "T_final, t_switch = t_forward + t_return, t_forward\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CV SIMULATION: ADAPTIVE LOSS WEIGHTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Finite difference\n",
    "del_x, del_t = L / (npts_x - 1), T_final / (npts_t - 1)\n",
    "x_grid = np.linspace(0, L, npts_x)\n",
    "t_grid = np.linspace(0, T_final, npts_t)\n",
    "lambda_fd = D * del_t / del_x**2\n",
    "\n",
    "def E_of_t_np(tt):\n",
    "    return E_start - sweep_rate_V_per_s * tt if tt <= t_switch else E_vertex + sweep_rate_V_per_s * (tt - t_switch)\n",
    "\n",
    "def run_finite_difference(c_O_bulk, c_R_bulk):\n",
    "    c_R = np.full((npts_x, npts_t), c_R_bulk, dtype=float)\n",
    "    c_O = np.full((npts_x, npts_t), c_O_bulk, dtype=float)\n",
    "    E_t, i_t = np.zeros(npts_t), np.zeros(npts_t)\n",
    "    E_eq = E_0 + (R * T / (n * F)) * np.log(c_O_bulk / c_R_bulk)\n",
    "    M_to_mol_cm3 = 1e-3\n",
    "    \n",
    "    for j in range(npts_t):\n",
    "        E_curr = E_of_t_np(t_grid[j])\n",
    "        E_t[j], eta = E_curr, E_curr - E_eq\n",
    "        k_red = k_0 * np.exp((-alpha * n * F * eta) / (R * T))\n",
    "        k_ox = k_0 * np.exp(((1.0 - alpha) * n * F * eta) / (R * T))\n",
    "        \n",
    "        if j > 0:\n",
    "            c_R_prev, c_O_prev = c_R[:, j-1], c_O[:, j-1]\n",
    "            c_R[1:-1, j] = c_R_prev[1:-1] + lambda_fd * (c_R_prev[2:] - 2*c_R_prev[1:-1] + c_R_prev[:-2])\n",
    "            c_O[1:-1, j] = c_O_prev[1:-1] + lambda_fd * (c_O_prev[2:] - 2*c_O_prev[1:-1] + c_O_prev[:-2])\n",
    "            c_R[-1, j], c_O[-1, j] = c_R_bulk, c_O_bulk\n",
    "            c_R_pred = c_R_prev[0] + lambda_fd * (c_R_prev[1] - c_R_prev[0])\n",
    "            c_O_pred = c_O_prev[0] + lambda_fd * (c_O_prev[1] - c_O_prev[0])\n",
    "            S, k_sum = c_R_pred + c_O_pred, k_red + k_ox\n",
    "            if k_sum > 0:\n",
    "                C_R_eq = (k_red / k_sum) * S\n",
    "                c_R[0, j] = C_R_eq + (c_R_pred - C_R_eq) * np.exp(-k_sum * del_t)\n",
    "                c_O[0, j] = S - c_R[0, j]\n",
    "            else:\n",
    "                c_R[0, j], c_O[0, j] = c_R_pred, c_O_pred\n",
    "            c_R[0, j], c_O[0, j] = max(c_R[0, j], 0.0), max(c_O[0, j], 0.0)\n",
    "            r_net = k_red * c_O[0, j] - k_ox * c_R[0, j]\n",
    "            i_t[j] = n * F * A * M_to_mol_cm3 * r_net\n",
    "    \n",
    "    return {'c_R': c_R, 'c_O': c_O, 'E_t': E_t, 'i_t': i_t, 'E_eq': E_eq}\n",
    "\n",
    "print(\"\\nRunning FD...\")\n",
    "fd_results = [run_finite_difference(s['c_O'], s['c_R']) | {'system': s} for s in systems]\n",
    "print(\"✓ FD complete\")\n",
    "\n",
    "# Neural networks\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "l_p_fixed = 3.5e-3\n",
    "\n",
    "class PINN_Oxidized(nn.Module):\n",
    "    def __init__(self, w=50, d=5):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(2, w), nn.SiLU()]\n",
    "        for _ in range(d - 2):\n",
    "            layers += [nn.Linear(w, w), nn.SiLU()]\n",
    "        layers.append(nn.Linear(w, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xt):\n",
    "        x = xt[:, 0:1]\n",
    "        recovery = 1.0 - torch.exp(-x / torch.tensor(l_p_fixed, device=xt.device))\n",
    "        return F_nn.softplus(self.net(xt)) * recovery\n",
    "\n",
    "class PINN_Reduced(nn.Module):\n",
    "    def __init__(self, w=50, d=5):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(2, w), nn.SiLU()]\n",
    "        for _ in range(d - 2):\n",
    "            layers += [nn.Linear(w, w), nn.SiLU()]\n",
    "        layers.append(nn.Linear(w, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xt):\n",
    "        x = xt[:, 0:1]\n",
    "        decay = torch.exp(-x / torch.tensor(l_p_fixed, device=xt.device))\n",
    "        return F_nn.softplus(self.net(xt)) * decay\n",
    "\n",
    "# GradNorm weighting\n",
    "class GradNormWeighting:\n",
    "    def __init__(self, n=3, alpha=1.5):\n",
    "        self.alpha, self.n, self.initial = alpha, n, None\n",
    "        self.weights = torch.ones(n)\n",
    "    \n",
    "    def update(self, losses, params, lr=0.025):\n",
    "        if self.initial is None:\n",
    "            self.initial = torch.tensor([l.item() for l in losses])\n",
    "        \n",
    "        grad_norms = []\n",
    "        for loss in losses:\n",
    "            grads = torch.autograd.grad(loss, params, retain_graph=True, create_graph=False)\n",
    "            grad_norms.append(torch.sqrt(sum([(g**2).sum() for g in grads])))\n",
    "        grad_norms = torch.stack(grad_norms)\n",
    "        \n",
    "        mean_norm = grad_norms.mean()\n",
    "        loss_ratios = torch.tensor([l.item() / (self.initial[i] + 1e-8) for i, l in enumerate(losses)])\n",
    "        targets = mean_norm * (loss_ratios ** self.alpha)\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            if targets[i] > 0:\n",
    "                self.weights[i] *= (1 + lr * (targets[i] / (grad_norms[i] + 1e-8) - 1))\n",
    "                self.weights[i] = max(self.weights[i].item(), 0.01)\n",
    "        self.weights = self.weights * self.n / self.weights.sum()\n",
    "        return self.weights.clone()\n",
    "\n",
    "# Helper functions\n",
    "def create_interp(t_data, y_data, dev):\n",
    "    t_t = torch.tensor(t_data, dtype=torch.float32, device=dev)\n",
    "    y_t = torch.tensor(y_data, dtype=torch.float32, device=dev)\n",
    "    def interp(t_q):\n",
    "        tc = t_q.view(-1).clamp(min=t_t[0], max=t_t[-1])\n",
    "        idx = torch.bucketize(tc, t_t, right=False).clamp(1, len(t_t)-1)\n",
    "        i0, i1 = idx - 1, idx\n",
    "        w = (tc - t_t[i0]) / (t_t[i1] - t_t[i0] + 1e-12)\n",
    "        return (y_t[i0] + w * (y_t[i1] - y_t[i0])).view_as(t_q)\n",
    "    return interp\n",
    "\n",
    "def E_torch(t):\n",
    "    ts = torch.tensor(t_switch, dtype=t.dtype, device=t.device)\n",
    "    fwd = E_start - sweep_rate_V_per_s * t\n",
    "    bwd = E_vertex + sweep_rate_V_per_s * (t - ts)\n",
    "    return torch.where(t <= ts, fwd, bwd)\n",
    "\n",
    "def k_red_t(t, Eeq):\n",
    "    return k_0 * torch.exp((-alpha * n * F * (E_torch(t) - Eeq)) / (R * T))\n",
    "\n",
    "def k_ox_t(t, Eeq):\n",
    "    return k_0 * torch.exp(((1.0 - alpha) * n * F * (E_torch(t) - Eeq)) / (R * T))\n",
    "\n",
    "def compute_c(model, x, t, cb, csf, stype):\n",
    "    H = (t > 0).float()\n",
    "    cs = csf(t)\n",
    "    cf = torch.tensor(cb, dtype=t.dtype, device=t.device)\n",
    "    xt = torch.cat([x, t], dim=1)\n",
    "    cr = model(xt)\n",
    "    c0 = model(torch.cat([torch.zeros_like(t), t], dim=1))\n",
    "    cL = model(torch.cat([torch.full_like(t, L), t], dim=1))\n",
    "    if stype == 'O':\n",
    "        cn = ((cr - c0) / (cL - c0 + 1e-8)).clamp(0, 1)\n",
    "        ct = cs + (cf - cs) * cn\n",
    "    else:\n",
    "        cn = ((cr - cL) / (c0 - cL + 1e-8)).clamp(0, 1)\n",
    "        ct = cf + (cs - cf) * cn\n",
    "    return cf * (1 - H) + ct * H\n",
    "\n",
    "def compute_derivs(mO, mR, x, t, cOb, cRb, cOf, cRf):\n",
    "    xr = x.detach().clone().requires_grad_(True)\n",
    "    tr = t.detach().clone().requires_grad_(True)\n",
    "    cO = compute_c(mO, xr, tr, cOb, cOf, 'O')\n",
    "    cR = compute_c(mR, xr, tr, cRb, cRf, 'R')\n",
    "    ones = torch.ones_like(cO)\n",
    "    dcOdt = torch.autograd.grad(cO, tr, ones, create_graph=True)[0]\n",
    "    dcOdx = torch.autograd.grad(cO, xr, ones, create_graph=True)[0]\n",
    "    dcRdt = torch.autograd.grad(cR, tr, ones, create_graph=True)[0]\n",
    "    dcRdx = torch.autograd.grad(cR, xr, ones, create_graph=True)[0]\n",
    "    d2cOdx2 = torch.autograd.grad(dcOdx, xr, ones, create_graph=True)[0]\n",
    "    d2cRdx2 = torch.autograd.grad(dcRdx, xr, ones, create_graph=True)[0]\n",
    "    return {'cO': cO, 'dcOdt': dcOdt, 'dcOdx': dcOdx, 'd2cOdx2': d2cOdx2,\n",
    "            'cR': cR, 'dcRdt': dcRdt, 'dcRdx': dcRdx, 'd2cRdx2': d2cRdx2}\n",
    "\n",
    "def compute_losses(mO, mR, xi, ti, xb, tb, cOb, cRb, Eeq, cOf, cRf):\n",
    "    d = compute_derivs(mO, mR, xi, ti, cOb, cRb, cOf, cRf)\n",
    "    resO = d['dcOdt'] - D * d['d2cOdx2']\n",
    "    resR = d['dcRdt'] - D * d['d2cRdx2']\n",
    "    lp = (resO.pow(2).mean() + resR.pow(2).mean()) / 2\n",
    "    \n",
    "    db = compute_derivs(mO, mR, xb, tb, cOb, cRb, cOf, cRf)\n",
    "    M = 1e-3\n",
    "    rf = (k_red_t(tb, Eeq) * db['cO'] - k_ox_t(tb, Eeq) * db['cR']) * M\n",
    "    flxO = (-D * db['dcOdx']) - rf\n",
    "    flxR = (-D * db['dcRdx']) + rf\n",
    "    lf = (flxO.pow(2).mean() + flxR.pow(2).mean()) / 2\n",
    "    \n",
    "    lc = (d['cO'] + d['cR'] - (cOb + cRb)).pow(2).mean()\n",
    "    return lp, lf, lc\n",
    "\n",
    "# Training setup\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING WITH GRADNORM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "N_int, N_bc, epochs, lr = 4096, 1024, 9000, 1e-3\n",
    "torch.manual_seed(42)\n",
    "\n",
    "pinn_models = []\n",
    "for i, sys in enumerate(systems):\n",
    "    torch.manual_seed(42 + i)\n",
    "    mO, mR = PINN_Oxidized().to(DEVICE), PINN_Reduced().to(DEVICE)\n",
    "    cOf = create_interp(t_grid, fd_results[i]['c_O'][0, :], DEVICE)\n",
    "    cRf = create_interp(t_grid, fd_results[i]['c_R'][0, :], DEVICE)\n",
    "    params = list(mO.parameters()) + list(mR.parameters())\n",
    "    opt = torch.optim.Adam(params, lr=lr)\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=1000, eta_min=1e-5)\n",
    "    gn = GradNormWeighting(n=3, alpha=1.5)\n",
    "    pinn_models.append({'O': mO, 'R': mR, 'system': sys, 'cOf': cOf, 'cRf': cRf,\n",
    "                       'opt': opt, 'sch': sch, 'gn': gn, 'hist': []})\n",
    "\n",
    "print(f\"Training {len(pinn_models)} systems for {epochs} epochs...\")\n",
    "\n",
    "for step in range(epochs + 1):\n",
    "    for i, (md, fdr) in enumerate(zip(pinn_models, fd_results)):\n",
    "        xi = torch.rand(N_int, 1, device=DEVICE) * L\n",
    "        ti = torch.rand(N_int, 1, device=DEVICE) * T_final\n",
    "        xb = torch.zeros(N_bc, 1, device=DEVICE)\n",
    "        tb = torch.rand(N_bc, 1, device=DEVICE) * T_final\n",
    "        \n",
    "        sys = md['system']\n",
    "        lp, lf, lc = compute_losses(md['O'], md['R'], xi, ti, xb, tb,\n",
    "                                    sys['c_O'], sys['c_R'], fdr['E_eq'],\n",
    "                                    md['cOf'], md['cRf'])\n",
    "        losses = [lp, lf, lc]\n",
    "        \n",
    "        if step % 10 == 0 and step > 0:\n",
    "            sp = list(md['O'].parameters())[:2]\n",
    "            w = md['gn'].update(losses, sp)\n",
    "            lt = sum(wi * li for wi, li in zip(w, losses))\n",
    "        else:\n",
    "            w = torch.tensor([1.0, 10.0, 1.0])\n",
    "            lt = w[0]*lp + w[1]*lf + w[2]*lc\n",
    "        \n",
    "        md['opt'].zero_grad()\n",
    "        lt.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(md['O'].parameters()) + list(md['R'].parameters()), 1.0)\n",
    "        md['opt'].step()\n",
    "        md['sch'].step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            wn = w.detach().cpu().numpy() if isinstance(w, torch.Tensor) else w.numpy()\n",
    "            md['hist'].append({'step': step, 'total': lt.item(), 'physics': lp.item(),\n",
    "                             'flux': lf.item(), 'conservation': lc.item(), 'weights': wn.copy()})\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        al = np.mean([h[-1]['total'] for m in pinn_models for h in [m['hist']] if h])\n",
    "        print(f\"  Step {step:5d} | Avg Loss: {al:.2e}\")\n",
    "\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for md in pinn_models:\n",
    "    md['O'].eval()\n",
    "    md['R'].eval()\n",
    "\n",
    "Nt, Nx = 500, 120\n",
    "te = torch.linspace(0, T_final, Nt, device=DEVICE).view(-1, 1)\n",
    "xe = torch.linspace(0, L, Nx, device=DEVICE).view(-1, 1)\n",
    "\n",
    "def eval_pinn(md, fdr):\n",
    "    sys = md['system']\n",
    "    with torch.no_grad():\n",
    "        x0 = torch.zeros_like(te)\n",
    "        cOs = compute_c(md['O'], x0, te, sys['c_O'], md['cOf'], 'O')\n",
    "        cRs = compute_c(md['R'], x0, te, sys['c_R'], md['cRf'], 'R')\n",
    "        M = 1e-3\n",
    "        kr = k_red_t(te, fdr['E_eq'])\n",
    "        ko = k_ox_t(te, fdr['E_eq'])\n",
    "        rn = kr * cOs - ko * cRs\n",
    "        ip = n * F * A * M * rn\n",
    "        Ep = E_torch(te)\n",
    "        \n",
    "        mask = (Ep >= -0.8) & (Ep < 0.1)\n",
    "        im = ip.clone()\n",
    "        im[~mask] = float('nan')\n",
    "        \n",
    "        if sys['ratio'] >= 1.0:\n",
    "            tmp = torch.where(torch.isnan(im), torch.tensor(float('-inf'), device=im.device), im)\n",
    "            ipk = torch.argmax(tmp).item()\n",
    "        else:\n",
    "            tmp = torch.where(torch.isnan(im), torch.tensor(float('inf'), device=im.device), im)\n",
    "            ipk = torch.argmin(tmp).item()\n",
    "        \n",
    "        ib = max(0, int(0.8 * ipk))\n",
    "        ia = min(Nt - 1, int(1.2 * ipk))\n",
    "        \n",
    "        profs = {}\n",
    "        for lbl, idx in zip(['t1', 't2', 't3'], [ib, ipk, ia]):\n",
    "            tp = te[idx].repeat(Nx, 1)\n",
    "            cOp = compute_c(md['O'], xe, tp, sys['c_O'], md['cOf'], 'O')\n",
    "            cRp = compute_c(md['R'], xe, tp, sys['c_R'], md['cRf'], 'R')\n",
    "            profs[lbl] = {'cO': cOp.cpu().numpy(), 'cR': cRp.cpu().numpy(),\n",
    "                         't': te[idx].item(), 'E': Ep[idx].item()}\n",
    "        \n",
    "        return {'E': Ep.cpu().numpy().flatten(), 'i': ip.cpu().numpy().flatten(),\n",
    "                't': te.cpu().numpy().flatten(), 'profiles': profs, 'indices': [ib, ipk, ia],\n",
    "                'k_red': kr.cpu().numpy().flatten(), 'k_ox': ko.cpu().numpy().flatten()}\n",
    "\n",
    "pinn_results = [eval_pinn(md, fdr) | {'system': md['system']} for md, fdr in zip(pinn_models, fd_results)]\n",
    "\n",
    "# Plotting\n",
    "print(\"\\nGenerating plots...\")\n",
    "plt.rcParams.update({'font.size': 11, 'axes.labelsize': 12, 'axes.titlesize': 13,\n",
    "                    'legend.fontsize': 10, 'figure.dpi': 150, 'lines.linewidth': 2.5})\n",
    "\n",
    "x_mm = xe.cpu().numpy().flatten() * 10\n",
    "x_mm_fd = x_grid * 10\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "prof_colors = ['#d62728', '#2ca02c', '#9467bd']\n",
    "\n",
    "# Plot 1: Voltammograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, (ax, fdr, pr) in enumerate(zip(axes, fd_results, pinn_results)):\n",
    "    sys = fdr['system']\n",
    "    mfd = (fdr['E_t'] >= -0.8) & (fdr['E_t'] < 0.1)\n",
    "    mpn = (pr['E'] >= -0.8) & (pr['E'] < 0.1)\n",
    "    ax.plot(fdr['E_t'][mfd], fdr['i_t'][mfd]*1e6, '--', lw=3, label='FD', color='gray', alpha=0.6)\n",
    "    ax.plot(pr['E'][mpn], pr['i'][mpn]*1e6, lw=2.5, label='PINN', color=colors[i])\n",
    "    for idx, col in zip(pr['indices'], prof_colors):\n",
    "        if -0.8 <= pr['E'][idx] < 0.1:\n",
    "            ax.axvline(pr['E'][idx], color=col, lw=1.5, alpha=0.5)\n",
    "    ax.set_xlabel('Potential (V)', fontweight='bold')\n",
    "    ax.set_ylabel('Current (μA)', fontweight='bold')\n",
    "    ax.set_title(f\"{sys['name']}: c_O/c_R = {sys['ratio']}\", fontweight='bold')\n",
    "    ax.set_xlim(-0.8, 0.1)\n",
    "    ax.axhline(0, color='k', ls='--', alpha=0.3, lw=1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_voltammogram_comparison_all_systems.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('01_voltammogram_comparison_all_systems.pdf', bbox_inches='tight')\n",
    "print(\"  ✓ 01_voltammogram_comparison\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Profiles\n",
    "for i, (fdr, pr) in enumerate(zip(fd_results, pinn_results)):\n",
    "    sys = fdr['system']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f\"{sys['name']}: Concentration Profiles and Dynamics\", fontsize=15, fontweight='bold')\n",
    "    \n",
    "    # Potential\n",
    "    axes[0, 0].plot(pr['t'], pr['E'], lw=2, color=colors[i])\n",
    "    axes[0, 0].axhline(E_0, ls='--', color='k', lw=1.2, label='E₀')\n",
    "    axes[0, 0].axhline(fdr['E_eq'], ls=':', color='red', lw=1.5, label='E_eq')\n",
    "    axes[0, 0].axvline(t_switch, ls='--', color='gray', lw=1, label='Switch')\n",
    "    for pf in pr['profiles'].values():\n",
    "        axes[0, 0].axvline(pf['t'], ls=':', alpha=0.5, lw=1)\n",
    "    axes[0, 0].set_xlabel('Time (s)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Potential (V)', fontweight='bold')\n",
    "    axes[0, 0].set_title('Potential Waveform', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.2)\n",
    "    \n",
    "    # Rate constants (dummy - not computed in eval)\n",
    "    #[0, 1].text(0.5, 0.5, 'Rate Constants\\n(see training)', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(pr['t'], pr['k_red'], linewidth=2, \n",
    "                label='k_red', color='#1f77b4')\n",
    "    ax.plot(pr['t'], pr['k_ox'], linewidth=2,\n",
    "                label='k_ox', color='#ff7f0e')\n",
    "    ax.axvline(t_switch, linestyle='--', color='gray', linewidth=1, alpha=0.5)\n",
    "    ax.set_xlabel('Time (s)', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Time (s)', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Rate Constant (cm/s)', fontweight='bold')\n",
    "    axes[0, 1].set_title('Rate Constants', fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.2)\n",
    "    \n",
    "    # c_R profiles\n",
    "    for j, (lbl, pf) in enumerate(pr['profiles'].items()):\n",
    "        ifd = np.argmin(np.abs(t_grid - pf['t']))\n",
    "        axes[1, 0].plot(x_mm_fd, fdr['c_R'][:, ifd] / sys['c_R'], '--', lw=2, color=prof_colors[j], alpha=0.5)\n",
    "        axes[1, 0].plot(x_mm, pf['cR'] / sys['c_R'], lw=2.5, color=prof_colors[j],\n",
    "                       label=f\"t={pf['t']:.2f}s (E={pf['E']:.2f}V)\")\n",
    "    axes[1, 0].axhline(1.0, ls='--', color='k', lw=1.2)\n",
    "    axes[1, 0].set_xlabel('Distance from Electrode (mm)', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('c_R / c_R,bulk', fontweight='bold')\n",
    "    axes[1, 0].set_title('Reduced Species', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.2)\n",
    "    \n",
    "    # c_O profiles\n",
    "    for j, (lbl, pf) in enumerate(pr['profiles'].items()):\n",
    "        ifd = np.argmin(np.abs(t_grid - pf['t']))\n",
    "        axes[1, 1].plot(x_mm_fd, fdr['c_O'][:, ifd] / sys['c_O'], '--', lw=2, color=prof_colors[j], alpha=0.5)\n",
    "        axes[1, 1].plot(x_mm, pf['cO'] / sys['c_O'], lw=2.5, color=prof_colors[j],\n",
    "                       label=f\"t={pf['t']:.2f}s (E={pf['E']:.2f}V)\")\n",
    "    axes[1, 1].axhline(1.0, ls='--', color='k', lw=1.2)\n",
    "    axes[1, 1].set_xlabel('Distance from Electrode (mm)', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('c_O / c_O,bulk', fontweight='bold')\n",
    "    axes[1, 1].set_title('Oxidized Species', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'02_profiles_system{i+1}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f'02_profiles_system{i+1}.pdf', bbox_inches='tight')\n",
    "    print(f\"  ✓ 02_profiles_system{i+1}\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot 3: Training loss\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, (ax, md) in enumerate(zip(axes, pinn_models)):\n",
    "    h = md['hist']\n",
    "    if not h:\n",
    "        continue\n",
    "    steps = [hi['step'] for hi in h]\n",
    "    ax.semilogy(steps, [hi['total'] for hi in h], lw=2.5, label='Total', color=colors[i])\n",
    "    ax.semilogy(steps, [hi['physics'] for hi in h], lw=1.5, label='Physics', alpha=0.7)\n",
    "    ax.semilogy(steps, [hi['flux'] for hi in h], lw=1.5, label='Flux', alpha=0.7)\n",
    "    ax.semilogy(steps, [hi['conservation'] for hi in h], lw=1.5, label='Conservation', alpha=0.7)\n",
    "    ax.set_xlabel('Training Step', fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontweight='bold')\n",
    "    ax.set_title(f\"{md['system']['name']} Training Loss\", fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_training_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('03_training_loss_curves.pdf', bbox_inches='tight')\n",
    "print(\"  ✓ 03_training_loss_curves\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 4: Overlay\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for i, fdr in enumerate(fd_results):\n",
    "    sys = fdr['system']\n",
    "    mfd = (fdr['E_t'] >= -0.8) & (fdr['E_t'] < 0.1)\n",
    "    ax1.plot(fdr['E_t'][mfd], fdr['i_t'][mfd]*1e6, lw=2.5,\n",
    "            label=f\"{sys['name']} (c_O/c_R={sys['ratio']})\", color=colors[i])\n",
    "\n",
    "ax1.set_xlabel('Potential (V)', fontweight='bold')\n",
    "ax1.set_ylabel('Current (μA)', fontweight='bold')\n",
    "ax1.set_title('Finite Difference: All Systems', fontweight='bold', fontsize=14)\n",
    "ax1.axhline(0, color='k', ls='--', alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.2)\n",
    "\n",
    "for i, pr in enumerate(pinn_results):\n",
    "    sys = pr['system']\n",
    "    mpn = (pr['E'] >= -0.8) & (pr['E'] < 0.1)\n",
    "    ax2.plot(pr['E'][mpn], pr['i'][mpn]*1e6, lw=2.5,\n",
    "            label=f\"{sys['name']} (c_O/c_R={sys['ratio']})\", color=colors[i])\n",
    "\n",
    "ax2.set_xlabel('Potential (V)', fontweight='bold')\n",
    "ax2.set_ylabel('Current (μA)', fontweight='bold')\n",
    "ax2.set_title('PINN: All Systems', fontweight='bold', fontsize=14)\n",
    "ax2.axhline(0, color='k', ls='--', alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_all_systems_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('04_all_systems_overlay.pdf', bbox_inches='tight')\n",
    "print(\"  ✓ 04_all_systems_overlay\")\n",
    "plt.close()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMULATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (fdr, pr) in enumerate(zip(fd_results, pinn_results)):\n",
    "    sys = fdr['system']\n",
    "    print(f\"\\n{sys['name']} (c_O={sys['c_O']}, c_R={sys['c_R']}, ratio={sys['ratio']}):\")\n",
    "    print(f\"  FD Peak Current:   {np.max(np.abs(fdr['i_t']))*1e6:.2f} μA\")\n",
    "    print(f\"  PINN Peak Current: {np.max(np.abs(pr['i']))*1e6:.2f} μA\")\n",
    "    err = np.abs(np.max(np.abs(pr['i'])) - np.max(np.abs(fdr['i_t']))) / np.max(np.abs(fdr['i_t'])) * 100\n",
    "    print(f\"  Relative Error:    {err:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL PLOTS GENERATED WITH ADAPTIVE WEIGHTING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  01_voltammogram_comparison_all_systems.png/pdf\")\n",
    "print(\"  02_profiles_system1.png/pdf\")\n",
    "print(\"  02_profiles_system2.png/pdf\")\n",
    "print(\"  02_profiles_system3.png/pdf\")\n",
    "print(\"  03_training_loss_curves.png/pdf\")\n",
    "print(\"  04_all_systems_overlay.png/pdf\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKEY IMPROVEMENTS:\")\n",
    "print(\"  1. GradNorm: Automatically balances loss gradients\")\n",
    "print(\"  2. Cosine annealing: Smooth learning rate decay\")\n",
    "print(\"  3. Gradient clipping: Prevents exploding gradients\")\n",
    "print(\"  4. Adaptive weights: Updated every 10 steps\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
