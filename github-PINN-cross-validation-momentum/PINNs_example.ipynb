{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLUzW3WTJKYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e9f9064-61fc-4220-dd41-1a1aedbc26d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cuda\n",
            "Step 1. Generate Data\n",
            "Step 2. Set Up Training\n",
            "layers: [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
            "OrderedDict({'layer_0': Linear(in_features=1, out_features=20, bias=True), 'activation_0': Tanh(), 'layer_1': Linear(in_features=20, out_features=20, bias=True), 'activation_1': Tanh(), 'layer_2': Linear(in_features=20, out_features=20, bias=True), 'activation_2': Tanh(), 'layer_3': Linear(in_features=20, out_features=20, bias=True), 'activation_3': Tanh(), 'layer_4': Linear(in_features=20, out_features=20, bias=True), 'activation_4': Tanh(), 'layer_5': Linear(in_features=20, out_features=20, bias=True), 'activation_5': Tanh(), 'layer_6': Linear(in_features=20, out_features=20, bias=True), 'activation_6': Tanh(), 'layer_7': Linear(in_features=20, out_features=20, bias=True), 'activation_7': Tanh(), 'layer_8': Linear(in_features=20, out_features=1, bias=True)})\n",
            "It: 0, Loss: 7.173e-01 , loss_mse: 7.157e-01,  r: 0.500, K: 0.499900\n",
            "It: 100, Loss: 3.733e-01 , loss_mse: 3.716e-01,  r: 0.489, K: 0.486182\n",
            "It: 200, Loss: 1.792e-01 , loss_mse: 1.609e-01,  r: 0.478, K: 0.495917\n",
            "It: 300, Loss: 1.564e-01 , loss_mse: 1.345e-01,  r: 0.458, K: 0.514795\n",
            "It: 400, Loss: 1.174e-01 , loss_mse: 9.022e-02,  r: 0.443, K: 0.528465\n",
            "It: 500, Loss: 8.917e-02 , loss_mse: 4.657e-02,  r: 0.426, K: 0.542834\n",
            "It: 600, Loss: 7.741e-02 , loss_mse: 3.729e-02,  r: 0.410, K: 0.556627\n",
            "It: 700, Loss: 6.883e-02 , loss_mse: 3.186e-02,  r: 0.396, K: 0.568506\n",
            "It: 800, Loss: 6.256e-02 , loss_mse: 2.838e-02,  r: 0.384, K: 0.579163\n",
            "It: 900, Loss: 5.765e-02 , loss_mse: 2.576e-02,  r: 0.372, K: 0.588976\n",
            "It: 1000, Loss: 5.345e-02 , loss_mse: 2.343e-02,  r: 0.362, K: 0.598181\n",
            "It: 1100, Loss: 4.969e-02 , loss_mse: 2.125e-02,  r: 0.352, K: 0.606862\n",
            "It: 1200, Loss: 4.627e-02 , loss_mse: 1.922e-02,  r: 0.342, K: 0.615058\n",
            "It: 1300, Loss: 4.314e-02 , loss_mse: 1.736e-02,  r: 0.333, K: 0.622841\n",
            "It: 1400, Loss: 4.026e-02 , loss_mse: 1.568e-02,  r: 0.324, K: 0.630258\n",
            "It: 1500, Loss: 3.759e-02 , loss_mse: 1.415e-02,  r: 0.315, K: 0.637343\n",
            "It: 1600, Loss: 3.513e-02 , loss_mse: 1.278e-02,  r: 0.307, K: 0.644124\n",
            "It: 1700, Loss: 3.284e-02 , loss_mse: 1.154e-02,  r: 0.299, K: 0.650622\n",
            "It: 1800, Loss: 3.072e-02 , loss_mse: 1.041e-02,  r: 0.291, K: 0.656857\n",
            "It: 1900, Loss: 2.875e-02 , loss_mse: 9.400e-03,  r: 0.283, K: 0.662844\n",
            "It: 2000, Loss: 2.692e-02 , loss_mse: 8.483e-03,  r: 0.276, K: 0.668599\n",
            "It: 2100, Loss: 2.522e-02 , loss_mse: 7.654e-03,  r: 0.268, K: 0.674133\n",
            "It: 2200, Loss: 2.363e-02 , loss_mse: 6.907e-03,  r: 0.261, K: 0.679460\n",
            "It: 2300, Loss: 2.216e-02 , loss_mse: 6.235e-03,  r: 0.254, K: 0.684590\n",
            "It: 2400, Loss: 2.080e-02 , loss_mse: 5.633e-03,  r: 0.248, K: 0.689533\n",
            "It: 2500, Loss: 1.954e-02 , loss_mse: 5.096e-03,  r: 0.241, K: 0.694298\n",
            "It: 2600, Loss: 1.837e-02 , loss_mse: 4.619e-03,  r: 0.235, K: 0.698894\n",
            "It: 2700, Loss: 1.730e-02 , loss_mse: 4.197e-03,  r: 0.229, K: 0.703328\n",
            "It: 2800, Loss: 1.630e-02 , loss_mse: 3.825e-03,  r: 0.223, K: 0.707608\n",
            "It: 2900, Loss: 1.539e-02 , loss_mse: 3.497e-03,  r: 0.217, K: 0.711741\n",
            "It: 3000, Loss: 1.454e-02 , loss_mse: 3.208e-03,  r: 0.211, K: 0.715733\n",
            "It: 3100, Loss: 1.376e-02 , loss_mse: 2.951e-03,  r: 0.205, K: 0.719592\n",
            "It: 3200, Loss: 1.303e-02 , loss_mse: 2.723e-03,  r: 0.200, K: 0.723322\n",
            "It: 3300, Loss: 1.235e-02 , loss_mse: 2.520e-03,  r: 0.195, K: 0.726930\n",
            "It: 3400, Loss: 1.173e-02 , loss_mse: 2.339e-03,  r: 0.190, K: 0.730422\n",
            "It: 3500, Loss: 1.114e-02 , loss_mse: 2.176e-03,  r: 0.185, K: 0.733801\n",
            "It: 3600, Loss: 1.059e-02 , loss_mse: 2.030e-03,  r: 0.180, K: 0.737073\n",
            "It: 3700, Loss: 1.009e-02 , loss_mse: 1.899e-03,  r: 0.175, K: 0.740243\n",
            "It: 3800, Loss: 9.611e-03 , loss_mse: 1.781e-03,  r: 0.170, K: 0.743314\n",
            "It: 3900, Loss: 9.168e-03 , loss_mse: 1.675e-03,  r: 0.166, K: 0.746291\n",
            "It: 4000, Loss: 8.755e-03 , loss_mse: 1.580e-03,  r: 0.161, K: 0.749177\n",
            "It: 4100, Loss: 8.368e-03 , loss_mse: 1.494e-03,  r: 0.157, K: 0.751976\n",
            "It: 4200, Loss: 8.007e-03 , loss_mse: 1.417e-03,  r: 0.153, K: 0.754692\n",
            "It: 4300, Loss: 7.670e-03 , loss_mse: 1.348e-03,  r: 0.149, K: 0.757327\n",
            "It: 4400, Loss: 7.354e-03 , loss_mse: 1.285e-03,  r: 0.145, K: 0.759886\n",
            "It: 4500, Loss: 7.060e-03 , loss_mse: 1.229e-03,  r: 0.141, K: 0.762369\n",
            "It: 4600, Loss: 6.784e-03 , loss_mse: 1.179e-03,  r: 0.137, K: 0.764781\n",
            "It: 4700, Loss: 6.526e-03 , loss_mse: 1.133e-03,  r: 0.133, K: 0.767124\n",
            "It: 4800, Loss: 6.285e-03 , loss_mse: 1.093e-03,  r: 0.129, K: 0.769401\n",
            "It: 4900, Loss: 6.060e-03 , loss_mse: 1.056e-03,  r: 0.126, K: 0.771613\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import warnings\n",
        "from scipy.integrate import odeint\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Device is {device}\")\n",
        "\"\"\"## Physics-informed Neural Networks\"\"\"\n",
        "\n",
        "\n",
        "# vectorize_time_dep_param = np.vectorize(time_dep_param)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def logistic_growth(N, t, r, K):\n",
        "    return r * N * (1 - N/K)\n",
        "\n",
        "\n",
        "# Step 1. Generate Data\n",
        "print(\"Step 1. Generate Data\")\n",
        "\n",
        "max_t = 30\n",
        "step_size = 1.0\n",
        "noise = 0.03\n",
        "\n",
        "t = np.arange(0, max_t, step_size) #0.1\n",
        "p_ = (0.3, 1.2)\n",
        "\n",
        "ic = 0.1\n",
        "# X is ideal data\n",
        "X = odeint(logistic_growth, ic, t, args=p_)\n",
        "\n",
        "# Add noise according to the mean\n",
        "xbar = np.mean(X, axis=0)\n",
        "Xn = X + noise * xbar * np.random.randn(*X.shape)\n",
        "\n",
        "    # Step 2. Setup Training\n",
        "print(\"Step 2. Set Up Training\")\n",
        "\n",
        "t_train = np.expand_dims(t.copy(),axis=1)\n",
        "t_test = np.arange(np.min(t_train),np.max(t_train)+(t_train[1]-t_train[0])/2,(t_train[1]-t_train[0])/2)\n",
        "t_colloc = np.expand_dims(np.arange(0, max_t, 0.001),axis=1)\n",
        "y_train = Xn.copy() # changed, used to be X\n",
        "\n",
        "# plot the data for sanity check\n",
        "plt.grid()\n",
        "plt.title(\"Trajectories\")\n",
        "plt.plot(t_train, y_train[:, 0], 'o',label='cell count',\n",
        "            color='k')\n",
        "plt.legend()\n",
        "plt.savefig('data')\n",
        "plt.clf()\n",
        "\n",
        "# Normalize inputs and outputs to be in [0, 1]\n",
        "tm, tM = t_train.min(), t_train.max()\n",
        "ym, yM = y_train.min(axis=0), y_train.max(axis=0)\n",
        "\n",
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers, min_val, max_val):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        self.min_val = torch.tensor([min_val], requires_grad=True).float().to(device)\n",
        "        self.max_val = torch.tensor([max_val], requires_grad=True).float().to(device)\n",
        "\n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "        print(layerDict)\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = (x - self.min_val) / (self.max_val - self.min_val)\n",
        "        out = self.layers(res)\n",
        "        return out\n",
        "\n",
        "max_lr = -1\n",
        "min_lr = -3\n",
        "\n",
        "time_delta = 30000\n",
        "warm_ups = 1000\n",
        "rate = (min_lr - max_lr) / time_delta\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Linear annealing LR schedule.\"\"\"\n",
        "    if epoch < warm_ups:\n",
        "        return 10**(max_lr)\n",
        "    elif epoch < time_delta:\n",
        "        return 10**(rate * (epoch - warm_ups) + max_lr)\n",
        "    else:\n",
        "        return 10**min_lr\n",
        "\n",
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, t, t_colloc, u_data, p_, layers, lb, ub):\n",
        "\n",
        "        # # boundary conditions\n",
        "        # self.lb = torch.tensor(lb).float().to(device)\n",
        "        # self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # data\n",
        "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.t = torch.tensor(t, requires_grad=True).float().to(device)\n",
        "        self.t_colloc = torch.tensor(t_colloc, requires_grad=True).float().to(device)\n",
        "        self.u_data = torch.tensor(u_data, requires_grad=True).float().to(device)\n",
        "\n",
        "        r, K = p_\n",
        "        self.r = torch.tensor([r], requires_grad=True).float().to(device) # parameters\n",
        "        self.K = torch.tensor([K], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.r = torch.nn.Parameter(self.r)\n",
        "        self.K = torch.nn.Parameter(self.K)\n",
        "\n",
        "        # settings\n",
        "        #self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
        "        #self.lambda_2 = torch.tensor([0.001*np.pi], requires_grad=True).to(device)\n",
        "\n",
        "        # deep neural networks\n",
        "        print(f\"layers: {layers}\")\n",
        "        #print(f\"tm: {tm}\")\n",
        "        #print(f\"tM: {tM}\")\n",
        "        self.dnn = DNN(layers,tm,tM).to(device)\n",
        "        self.dnn.register_parameter('r', self.r)\n",
        "        self.dnn.register_parameter('K', self.K)\n",
        "\n",
        "         # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),#self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=0,#1e-8,\n",
        "            tolerance_change=0,#1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
        "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer_Adam, lr_lambda=lr_schedule)\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, t):\n",
        "        u = self.dnn(t)\n",
        "        return u\n",
        "\n",
        "    def net_f(self, t, t_colloc, u_data):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        # u = self.net_u(t)\n",
        "        u_colloc = self.net_u(t_colloc)\n",
        "\n",
        "        # loss_mse = torch.mean(torch.square(u - u_data))\n",
        "\n",
        "        _x = torch.unsqueeze(u_colloc[:, 0],axis=1)\n",
        "\n",
        "        x_t = torch.autograd.grad(\n",
        "            _x, t_colloc,\n",
        "            grad_outputs=torch.ones_like(_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True,\n",
        "        )[0]\n",
        "\n",
        "        x_dot = self.r * _x * (1 - _x/self.K)\n",
        "        loss_pinn = torch.mean(torch.square(x_t - x_dot))\n",
        "\n",
        "        return loss_pinn\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred = self.net_u(self.t)\n",
        "        pinn_loss = self.net_f(self.t, self.t_colloc, self.u_data)\n",
        "        data_loss = torch.mean((self.u_data - u_pred) ** 2)\n",
        "        loss = data_loss + pinn_loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Loss: %e, data_loss: %e, pinn_loss: %e, r: %.3f, K: %.6f' %\n",
        "                (\n",
        "                    loss.item(),\n",
        "                    data_loss.item(),\n",
        "                    pinn_loss.item(),\n",
        "                    self.r.item(),\n",
        "                    self.K.item()\n",
        "                )\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self, adam_epochs, bfgs_epochs, polish_adam_epochs):\n",
        "        self.dnn.train()\n",
        "\n",
        "        for epoch in range(adam_epochs):\n",
        "            # print(f\"self.t shape: {self.t.shape}\")\n",
        "            u_pred = self.net_u(self.t)\n",
        "            # print(f\"u_pred shape: {u_pred.shape}\")\n",
        "            # print(f\"u_data shape: {self.u_data.shape}\")\n",
        "            # print(f\"self.u_data - u_pred shape: {(self.u_data - u_pred).shape}\")\n",
        "            loss_pinn = self.net_f(self.t, self.t_colloc, self.u_data)\n",
        "            loss_mse = torch.mean(torch.square(self.u_data - u_pred))\n",
        "            #print(f\"loss_mse: {torch.mean(torch.square(self.u_data - u_pred))}\")\n",
        "            loss = loss_mse + loss_pinn #torch.mean(f_pred ** 2)\n",
        "            # print(f\"loss shape: {loss.shape}\")\n",
        "\n",
        "            # Backward and optimize\n",
        "            self.optimizer_Adam.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer_Adam.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(\n",
        "                    'It: %d, Loss: %.3e , loss_mse: %.3e,  r: %.3f, K: %.6f' %\n",
        "                    (\n",
        "                        epoch,\n",
        "                        loss.item(),\n",
        "                        loss_mse.item(),\n",
        "                        self.r.item(),\n",
        "                        self.K.item()\n",
        "                    )\n",
        "                )\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "    def predict(self, t):\n",
        "        #x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(t)\n",
        "        # f = self.F_net(t)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        # f = f.detach().cpu().numpy()\n",
        "        return u\n",
        "\n",
        "\"\"\"## Configurations\"\"\"\n",
        "\n",
        "N_u = 2000\n",
        "layers = [1, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "\"\"\"## Training on Non-noisy Data\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "#\n",
        "\n",
        "#\n",
        "#\n",
        "adam_epochs = 5000\n",
        "polish_adam_epochs = 2000\n",
        "bfgs_epochs = 2000\n",
        "\n",
        "# training\n",
        "model = PhysicsInformedNN(t_train, t_colloc, y_train, (0.5,0.5), layers, 0, max_t)\n",
        "model.train(adam_epochs, bfgs_epochs, polish_adam_epochs)\n",
        "\n",
        "# evaluations\n",
        "u_pred= model.predict(model.t)\n",
        "\n",
        "plt.title(\"Trajectories\")\n",
        "plt.plot(t_train, y_train[:, 0], 'o',label='Measurements',\n",
        "          color='k')\n",
        "plt.plot(t_train, u_pred[:, 0],'-', label='PINN approximation', color='r')\n",
        "plt.savefig('trajectories')\n",
        "plt.legend()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRfxjkPDnIHH",
        "outputId": "0976c59a-f4e5-4d81-d219-48d4713d0dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([1.2026], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}